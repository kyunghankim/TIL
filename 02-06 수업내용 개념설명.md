# 02-06 수업내용 개념설명

* 차원:

  머신러닝이란? 컴퓨터가 data로부터 이론을 만들도록하는 방법.

  ex: 

  사람이 사물을 정의할 때, 다각도로 바라봄.

  사과->색이 빨감, 씨가있음, 사과모양 등...

  x=색, y=씨,z=모양 으로 정의해서 3차원 공간에 data를 표시 가능.

  =>이 학문이 '선형대수'

* DATA에서)

  행 하나하나는 하나의 존재, 열은 특성을 의미.

  ex:

  동물-> 꼬리 유무,다리 갯수, 짖는지, 몸길이...

  컴퓨터: 1,4,1,50 =>개라고 추측

  ​             1,4,1,20 => 고양이 추측    (<-이렇게 학습을 시킴)

  ​          x=1,4,1,45=> 개일 확률이 높고 고양이일 확률 낮음 => 개라고 추측

    개, 고양이, 돼지 x 0.7 0.1 0.2  (<-딥러닝:softmax)

* 기계학습(머신러닝)

  1) 지도학습: 교사(문/답), 문=data  답=label(통상적으로label이라함)

  2)비지도학습: 답x , data만 있음

    k-mean,autoencoder(GAN)

  3) 강화학습: 잘 하면 칭찬, 못하면 벌을 주는 학습

* 지도학습(Supervised-learning)

  (정답)<-예측값(컴퓨터가만든)의 모습을 표현하는 용어: 편향/분산

  편향(Bias)/분산(Variance) 

  편향(오류): 예측값들과 정답이 어느정도 떨어져 있는지 나타낸 것

  ​                  => bias 높다=정답과 예측값의 차이가 크다.

  분산: 예측값들끼리 어느정도 떨어져 있는지 나타낸 것

  ​         => 떨어져 있느냐를 나타낸 것

  ​         => Variance 높다: 예측값들끼리 서로 멀리 떨어져있다.

* x:데이터, f(x):정답 f^(x)(=어떤 특정값):예측값, E[ ~ ]:~의 기댓값(평균)

  E[f^(x)]:예측값들의 평균

  => 편향의 정의: (E[f^(x)] - f(x) )^2  

​            (*제곱을 쓰는 이유는, 크기와 패널티를 줄 수 있기 때문)

​        => 분산의 정의: E[ f^(x)-E[f^(x)] ]^2

* x축:Error(x), y축:a 

  where Error(x): 편향+분산+e(<-irreducible error,근본적인 오류)

* overfitting(과적합) / underfitting(과소적합)

  (sketch)

  x축:x1 , y축:x2일때 실제 정답들의 점이 찍혀있음(data)

  선형회귀모델)

  -점들을 전부지나는 점은 편향은 낮음, 분산 높음(구불구불하니까):overfitting

  -일직선은 편향이 높음, 분산은 낮음(직선이니까): underfitting

  -한번 꺾이는 선은 편향 보통, 분산도 보통(한번만 꺾이니까): 적절한 모델

  =>앞으로는 우리가 적절한 모델을 만들어야 함

* 분류 문제:

  (x1,x2) 평면에 o,x점들이 있다고 했을 때

  o는 원점에 가깝고 x는 먼 경우, 

  1번 구분선은 기울기가 -1인 선으로 그어짐(편향은 높고, 분산은 낮다)

  2번 구분선은 모두 나눌수 있도록 구불구불하게 난어짐(편향은 낮고, 분산은 높음

  3번 한번만 꺾이면서 나누어지게 그어짐(적당한 편향,분산)

  => underfitting: 훈련이 덜 된 상태(모델 단순), overfitting:훈련이 심하게 됨(모델 복잡)

  =>편향과 분산은 반비례하는 경향이 있음, 반복학습하는 횟수가 늘 수록 복잡

  =>Training error는 줄어듬, Validation(검증용데이터)는 처음에 줄다 다시 올라감

  (sketch)

  x축:모델 복잡도, y축:Error일 때

  training데이터는 반비례, validation은 U모양으로 내려가다 올라감

  두 값의 차이를 Bias라고 함 

  => 딱 중간에서 가장 적어지는 지점을 찾는 것이 목표

  -overfitting을 없애는 방법은 없음!

* DATA를 7:3으로 나눈 후 7을 다시 7:3으로 나누어 traing data / validation data로 나눔 처음 3은 test data

  트레이닝을 많이하면 overfitting(과적합)

  =>모델이 훈련데이터에 대해서만 과하게 맞춰짐

  (1)training data로 모델 확정

  (2)validation data로 모델에 대해 과적합/과소적합인지 검증=>최적적합구간을 찾음

  => h(x)=wx+b (w:weight,b:bias)

  (3) test data:최종모델로 실제로 테스트

  

* K-fold validation: training data를 k개로 분리 => k-1개 data로 훈련, 나머지로 validation

  ​                               =>k-fold cross validation: 첫번째 뺴고 나머지로, 두번째 빼고 나머지로...

  (보통 데이터를 훈련/검증/테스트로 분할)

  case) data가 부족한 경우(나누기 까지하기 힘듬)

* 연관규칙(프로젝트에 추가하는) : 장바구니 분석이라고도 함

  -데이터들 사이의 연관성 찾기

  -거래 data에서 특성 찾기(pattern)

  -item집합(장바구니),item(상품): item{ 빵, 버터, 우유, 껌,... }, ...

  ​                                                        => 연관규칙(Apriori알고리즘) : LHS {버터,빵} => RHS {우유}

  -예) 암 data(DNA패턴,단백질 서열), 사기성 신용카드사용, 부당의료비(보험) 청구 패턴,

  변심? 선행되는 동작 패턴 식별(통신사 변경) 등...

* 시간의 흐름에 따른 구매 패턴:

  추천시스템: 1) 연관규칙을 활용한 추천:함께 구매가 발생하는 규칙

  ​                     2) C.F:상관계수를 기반으로 

  ​                     3) 순차분석: 시간의 흐름에 따른 구매패턴

  * 연관규칙(다른것들도):Data가 적으면 다 안됨, 규칙찾는건 되는데 통찰은 스스로 해야함(Domain에 대한 지식필요)

------------------------------------------------------------------------

평가 지표 3가지

* 지지도(Support): item 집합이 나타나는 트랜잭션(거래) 비율

  * x,y항목의 지지도=  (x,y를 모두 포함하고 있는 거래 수)  / 전체 거래 수 

  * ex:  (1) 계란, 우유, (2) 계란, 기저귀, 맥주, 사과 (3) 우유, 기저귀, 맥주, 콜라

    ​      (4) 계란, 우유, 맥주, 기저귀  (5) 계란, 우유, 맥주, 콜라

    => {계란,맥주}->{기저귀} 지지도 구하기

    x= {계란,맥주}, y={기저귀} 일 때 지지도=  2건(2,4번) / 전체 거래 수(5) = 0.4

* 신뢰도(Confidence): 항목 집합 X를 포함하는 거래 중에서, 항목집합 Y도 포함하는 거래비율

  (=> 조건부확률)

  ​    X와 Y를 모두 포함한 거래 수 / x가 포함된 거래수= n(X교집합Y) / n(X)

  * 위 ex:     2건(2,4번) / 3건(2,4,5번) =0.67

* 향상도(Lift): 

  항목집합 X가 주어져 있지 않은 상황에서, 항목집합 Y의 확률 대비 항목집합X가 주어졌을 때,

  Y의 확률 증가 비율

  향상도 = 신뢰도 / 지지도

  =>    C(onfidence)(X->Y) / S(upport)(Y)

  * 위 ex: S(Y)= 3건(기저귀)/5건(전체)=0.6, C(Y)=0.67

    L= C(Y)/S(Y)=1.11..

    => C(Y)가 높은 연관규칙을 찾는 것이 작업의 목표!!

*  X, Y (item들): X -> Y 연관규칙이 몇개 있을 까 ?

  ex:

  {a,b,c,d} 일 경우 4c2, 4c3등.. 만약 집합의 갯수가 만개이면 컴퓨터로 해야함

  => Apriory로 함: Frequency를 통해

  ​                         tree구조:

  ​                                          a

  ​                                 a  |  b  |  c  |   d

  ​                       ab,ac,ad|bc,bd|cd|

​                                 | ...|    ... | ...|

​                                       abcd

* Pruning(가지치기): 더이상 해가 될 가능성이 없다면, 애초에 제거

  --------------------------------------------

  복습:

* 최소지지도: ~이상을 갖는 집합(빈발항목 집합)

  ex: 최소지지도 0.5 => 전체 거래에서 어떤 항목을 보유한 거래가 50%이상인 항목들을 빈발항목 집합이라함

​           *아프리온? 모든항목 집합의 지지도(너무 많아서 안됨)가 아닌 빈발항목집합만을 찾아서 연관규칙 작용

​          X가 빈발항목집합일 경우 X의 부분집합도 빈발항목집합

​       ex: X={ 맥주, 소주, 땅콩, 오징어, 쥐포} 

​        지지도 0.3, X의 부분집합=2^5

​       => 지지도를 잘 정해야함 너무 높으면 해당 원소가 없을 수도

(2) 한 항목집합(item set)이 비빈발(<->frequent)하다면, 이 항목집합을 포함하는 모든 집합은 비빈발집합

=>이거를 programming

ex: {사과}의 지지도 0.1, 최소지지도를 0.3으로 정하면 {사과}는 비빈발집합

=>{사과,딸기}의 지지도는 0.1보다 작거나 같음 => 최소지지도보다도 작아짐

=> 한 항목이 비빈발항목이면 그 밑에있는 tree의 가지들을 전부 뺌







