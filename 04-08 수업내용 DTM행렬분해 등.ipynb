{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # <- natural language tool kit\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'treebank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7741bec97b85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreebank\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# <- 따로 설치, 잘 안돼서 직접 다운\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'treebank' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download(treebank) # <- 따로 설치, 잘 안돼서 직접 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화, 정제, 정규화\n",
    "#토큰화: 어절, 단어, 문장, 문단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize   # <- 단어별로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize   # <- 문장별로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"'s\", 'Do', \"n't\", 'be']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Mr. John's Don't be \")) #<-공백기준으로 다 나뉘고 ' 기준으로 나뉨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer # Punctuation: 구두점\n",
    "WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr', '.', 'John', \"'\", 's', 'Don', \"'\", 't', 'be']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(\"Mr. John's Don't be \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"'s\", 'Do', \"n't\", 'be']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Mr. John's Don't be \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr', \"john's\", \"don't\", 'be']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(\"Mr. John's Don't be \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#자연어처리: 정규표현식을 잘 알아야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Mr. John's Don't be \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'John', \"'s\", 'Do', \"n't\", 'be']\n"
     ]
    }
   ],
   "source": [
    "twt=TreebankWordTokenizer()\n",
    "print(twt.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is an interpreted, high-level, general-purpose programming language.', \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))\n",
    "#조사: she her 같은 단어들\n",
    "#한국어 토큰화 작업: 형태소(ex:가장 작은 말의단위) 고려해야함\n",
    "#자립형태소/의존형태소\n",
    "#ex: \"길동이가 파이썬을 합니다\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사 태깅(tagging): ex) fly \n",
    "#토큰화 과정에서 단어가 어떤 품사로 쓰였는지 구분하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Python is an interpreted, high-level, general-purpose programming language.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('interpreted', 'JJ'),\n",
       " (',', ','),\n",
       " ('high-level', 'JJ'),\n",
       " (',', ','),\n",
       " ('general-purpose', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(text)) #<- 품사별로 tagging이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNP: 단수 고유명사\n",
    "VB: 동사\n",
    "VBP: 동사 현재형\n",
    "TO: to 전치사\n",
    "NN: 명사(단수형 혹은 집합형)\n",
    "DT: 관형사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(우리나라)형태소 분석기: Okt, Mecab, Kkma, ... 등\n",
    "#우리나라말은 단어토큰화가 아님. 형태소 토큰화를 하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- okt.morphs로 출력:\n",
      " ['오늘', '은', '수요일', ',', '내일', '은', '목요일', '입니다', '.']\n",
      "- okt.pos로 출력:\n",
      " [('오늘', 'Noun'), ('은', 'Josa'), ('수요일', 'Noun'), (',', 'Punctuation'), ('내일', 'Noun'), ('은', 'Josa'), ('목요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n",
      "- okt.nouns로 출력:\n",
      " ['오늘', '수요일', '내일', '목요일']\n"
     ]
    }
   ],
   "source": [
    "okt=Okt()\n",
    "#morphs : 형태소 추출\n",
    "print(\"- okt.morphs로 출력:\\n\",okt.morphs(\"오늘은 수요일, 내일은 목요일입니다.\"))\n",
    "#pos : 품사 추출\n",
    "print(\"- okt.pos로 출력:\\n\",okt.pos(\"오늘은 수요일, 내일은 목요일입니다.\"))\n",
    "#nouns: 명사\n",
    "print(\"- okt.nouns로 출력:\\n\",okt.nouns(\"오늘은 수요일, 내일은 목요일입니다.\"))\n",
    "#konlpy.org 사이트 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kma=Kkma()\n",
    "kma.morphs()  # <- 이런식으로 사용하면됨 사용법은 비슷함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정제(전처리): 불필요한 형태소 제거, 단어 통일(밥,식사,...)\n",
    "#정규표현식을 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA (Latent Semantic Analysis)\n",
    "\"\"\" 문서: 1번  2번  3번              4번       5번           6번\n",
    "ex)     떡만두 김밥 떡만두+김밥    |   피자  햄버거,쿠키,피자   햄버거   <- 문서\n",
    " ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    " 떡만두   1    0     1\n",
    " 김밥     0   1      1\n",
    " 피자     0   0      0\n",
    " 햄버거   0   0      0\n",
    " 쿠키     0   0      0\n",
    " (^DTM)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "유사도(4번문서,6번문서)?\n",
    "=>cos유사도로 0이 나옴 (공통된단어가 없어서 내적하면 0이기때문)\n",
    "토픽모델링의 한계가 생김\n",
    "TF-IDF도 계산해봐야 0나옴\n",
    "이유: 단어를 기반으로 유사도를 구하기 때문\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LSA 사용하면 토픽으로 유사도를 구하기 때문에\n",
    "SVD(Singular Value Decomposition): 특이값 분해\n",
    "A행렬 = mxn 행렬, 3개의 행렬의 곱형태로 분해\n",
    "=>A=U*sigma*V^T  <-특이값 분해\n",
    "U:토픽에 대한 단어 행렬\n",
    "sigma: 토픽 강도\n",
    "V^T: 토픽에 대한 문서행렬\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A 행렬은 DTM\n",
    "A=np.array([[0,0,0,1,0,1,1,0,0],\n",
    "            [0,0,0,1,1,0,1,0,0],\n",
    "          [0,1,1,0,2,0,0,0,0],\n",
    "          [0,0,0,0,0,0,0,1,1]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, VT =np.linalg.svd(A,full_matrices=True)  # <- full SVD , truncted(절단된) SVD도있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75  0.    0.62]\n",
      " [-0.51  0.44  0.   -0.74]\n",
      " [-0.83 -0.49  0.    0.27]\n",
      " [ 0.    0.    1.    0.  ]] \n",
      " ------------------------------ \n",
      " \n",
      " [2.69 2.05 1.41 0.77] \n",
      " ------------------------------ \n",
      " [[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28  0.    0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.71  0.71]\n",
      " [-0.    0.35  0.35 -0.16 -0.25  0.8  -0.16  0.    0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.58  0.28 -0.69 -0.1   0.21  0.21 -0.1   0.    0.  ]\n",
      " [-0.58 -0.14  0.35 -0.45 -0.1  -0.1   0.55  0.    0.  ]\n",
      " [ 0.41  0.1  -0.25 -0.39  0.07  0.07  0.32  0.5  -0.5 ]\n",
      " [ 0.41  0.1  -0.25 -0.39  0.07  0.07  0.32 -0.5   0.5 ]]\n",
      "s는 diagonal elements만 나와서 아래와 같이 숫자4개임 바꿔줘야함\n"
     ]
    }
   ],
   "source": [
    "U=U.round(2) # <- 직교행렬\n",
    "s=s.round(2)\n",
    "VT=VT.round(2)\n",
    "print(U,\"\\n\", \"-\"*30,\"\\n\", \"\\n\",s,\"\\n\",\"-\"*30,\"\\n\",VT)\n",
    "print(\"s는 diagonal elements만 나와서 아래와 같이 숫자4개임 바꿔줘야함\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=np.zeros((4,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[:4,:4]=np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-특이값 행렬: \n",
      " [[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.41 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-특이값 행렬:\",'\\n',S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28  0.    0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.71  0.71]\n",
      " [-0.    0.35  0.35 -0.16 -0.25  0.8  -0.16  0.    0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.58  0.28 -0.69 -0.1   0.21  0.21 -0.1   0.    0.  ]\n",
      " [-0.58 -0.14  0.35 -0.45 -0.1  -0.1   0.55  0.    0.  ]\n",
      " [ 0.41  0.1  -0.25 -0.39  0.07  0.07  0.32  0.5  -0.5 ]\n",
      " [ 0.41  0.1  -0.25 -0.39  0.07  0.07  0.32 -0.5   0.5 ]] \n",
      " VT shape: (9, 9)\n"
     ]
    }
   ],
   "source": [
    "print(VT,\"\\n\",\"VT shape:\",VT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -0.   -0.    1.   -0.    1.01  1.    0.    0.  ]\n",
      " [ 0.    0.01  0.01  1.    1.01  0.    1.    0.    0.  ]\n",
      " [ 0.    1.01  1.01  0.01  2.   -0.    0.01  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    1.    1.  ]]\n",
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "dotUS=np.dot(U,S)\n",
    "print(np.dot(dotUS,VT).round(2))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A,np.dot(dotUS,VT).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "#절단된(truncted) SVD\n",
    "#토픽 2개\n",
    "S=S[:2,:2]\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특이값의 의미: 가장큰 값이 가장 큰 토픽을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape #(4,4)\n",
    "Utemp=U[:,:2] #<- (4,2) : 문서 4개, 토픽 2개\n",
    "#U의 의미는 4개 문서 각각에 대한 잠재된 의미를 표현하고있는 \n",
    "#수치 문서 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT.shape\n",
    "VTtemp=VT[:2,:] #<- (2,9) : 토픽 2개, 단어 9개\n",
    "# VT에서 각 열의 의미는 잠재 의미를 나타내기 위한 수치화된\n",
    "# 단어 벡터\n",
    "# 크기가 2인 행렬만: 가장 강한 두 가지만 뽑겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utemp*S*VTtemp !=A : A와는 다른 행렬로 나옴, 일부 손실했기 때문\n",
    "#하지만 A의 상당 부분을 담고있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -0.17 -0.17  1.07  0.12  0.63  1.07  0.    0.  ]\n",
      " [ 0.    0.21  0.21  0.91  0.86  0.46  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.04  2.05 -0.17  0.04  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]] \n",
      " ---------------------------------------- \n",
      " [[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "Aprime=np.dot(np.dot(Utemp,S),VTtemp)\n",
    "print(Aprime.round(2),'\\n','-'*40,'\\n',A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     -0.8339 -0.8339 -0.7532 -2.152  -0.2421 -0.7532  0.      0.    ]\n",
      " [ 0.     -0.492  -0.492   1.189  -0.533   0.7585  1.189   0.      0.    ]] (2, 9)\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(S,VTtemp),np.dot(S,VTtemp).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 토픽: [ 0.    -0.834 -0.834 -0.753 -2.152 -0.242 -0.753  0.     0.   ]\n",
      "두번째 토픽: [ 0.    -0.492 -0.492  1.189 -0.533  0.758  1.189  0.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "print(\"첫번째 토픽:\",np.dot(S,VTtemp).round(3)[0])\n",
    "print(\"두번째 토픽:\",np.dot(S,VTtemp).round(3)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
