# 02-10(9am) 개념설명 

-클러스터 중심간 각 data간 거리의 합 최소화

-클러스터 중심 간에 거리의 합 최대

(1) k결정

(2) k개 초기 중심 설정

(3) data와 중심(클러스터)이 가장 가까운 클러스터로 데이터 할당

(4) 중심점이 update

## Deicision tree 

*Entropy(복잡도,혼잡도):

Entropy가 높다=> 혼잡,복잡하다.

0~1값을 같는 entropy의 값이 1이면 높음

Entropy=-plog2(p)-(1-p)log2(1-p)

Information Gain=Entropy_before-Entropy_after

=>나누기 전과 후의 Entropy를 빼서 구함

(I.G가 높은 것을 찾음)

ex) 신용평가 모델, 고객행동, 만족도, 마케팅

제약회사 약제조,질병 진단,...

-----------------------------------------------------------

오후)

*<u>분할</u> 기준 선정

Entropy= sigma(i~c)[-p_i*log2(p_i)]

  ex) 빨간색(60%), 흰색(40%)

=-0.6*log2(0.6)-0.4*log2(0.4)

=0.9

*Entropy_before-Entropy_after

=> Entropy_after를 구할 때 가중평균으로 구하는 방법

Entropy_weighted=( n1x(n1'Entropy) + n2x(n2'Entropy) + n3x(n3'Entropy)) / (n1 + n2 + n3)

=> 보통 산술평균을 쓰기도 하지만 weighted로 할 때도 있음(데이터가 많은 경우)











